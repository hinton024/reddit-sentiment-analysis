{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ze5pdpaIG1q"
      },
      "source": [
        "# Assignment 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucHgpldkSav1",
        "outputId": "13ef2a8e-2a7c-4e03-f27b-8100e68f3776"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHbnRZR3IG1r"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport redditClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv7WXQ-tIG1s"
      },
      "source": [
        "# Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3zY8l27IG1s",
        "outputId": "b97f1b48-75cd-4647-ac35-118aa7384432"
      },
      "outputs": [],
      "source": [
        "# Importing packages and nltk data libraries\n",
        "import string\n",
        "import json\n",
        "import codecs\n",
        "import re\n",
        "import sys\n",
        "from collections import Counter\n",
        "from spellchecker import SpellChecker\n",
        "import functools\n",
        "import datetime\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import ijson\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.util import ngrams\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from collections import Counter\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from itertools import repeat\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "# Importing Reddit client and PRAW\n",
        "from redditClient import redditClient\n",
        "import praw\n",
        "from prawcore.exceptions import TooManyRequests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfldapeAIG1s",
        "outputId": "79c11c92-e867-41cc-ee62-18a1fc3ff641"
      },
      "outputs": [],
      "source": [
        "# subreddit name we interested in getting the hot submissions\n",
        "sSubredditName = 'NVDA_Stock'\n",
        "# maximum number of hot submissions\n",
        "hotLimit = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKlaG3BUIG1s"
      },
      "source": [
        "Construct Reddit client then print our name to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5kzODFWIG1s",
        "outputId": "a7a79e2f-e23d-4bc3-8ebf-3fe356ccd388"
      },
      "outputs": [],
      "source": [
        "# construct Reddit client\n",
        "client = redditClient()\n",
        "\n",
        "# sanity check, you should see your own username printed out\n",
        "print(client.user.me())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uVuty1RznF5"
      },
      "source": [
        "### Scrape reddit data through api and save it to json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_IFj4GvIG1t",
        "outputId": "b1f2569c-2a1c-47e2-d0ca-bde848b07a33"
      },
      "outputs": [],
      "source": [
        "@functools.lru_cache(maxsize=None)\n",
        "def reddit_api_response(sSubredditName, hotLimit,start_date, end_date):\n",
        "    # specify which subreddit we are interested in\n",
        "    subreddit = client.subreddit(sSubredditName)\n",
        "    # Convert dates to timestamps\n",
        "    start_timestamp = int(start_date.timestamp())\n",
        "    end_timestamp = int(end_date.timestamp())\n",
        "    data = []; comments_json = []\n",
        "    for submission in subreddit.top(limit=None):\n",
        "        try:\n",
        "            submission.comments.replace_more(limit=None) # expand all the comments\n",
        "            if start_timestamp <= submission.created_utc <= end_timestamp:\n",
        "                comments=[]\n",
        "                for comment in submission.comments.list():\n",
        "                    comment_data={\n",
        "                        \"author\": comment.author.name if comment.author else \"No Author\",\n",
        "                        \"created\": comment.created_utc,\n",
        "                        \"text\": comment.body\n",
        "                    }\n",
        "                    comments.append(comment_data)\n",
        "                post_data = {\n",
        "                \"title\": submission.title,\n",
        "                \"author\": submission.author.name if submission.author else \"No Author\",\n",
        "                \"score\": submission.score,\n",
        "                \"created\":submission.created_utc\n",
        "                }\n",
        "                data.append(post_data)\n",
        "                comments_json.extend(comments)\n",
        "            # else:\n",
        "            #     break\n",
        "        except TooManyRequests as e:\n",
        "            time.sleep(60)\n",
        "    print(data)\n",
        "    comments_json = {\"submissions\":comments_json}\n",
        "    final_data = {\"submissions\":data}\n",
        "    with open(f\"nvidia_stock_data_{start_date.strftime('%Y_%m_%d')}_{end_date.strftime('%Y_%m_%d')}.json\", \"w\") as json_file:\n",
        "        json.dump(final_data, json_file)\n",
        "    with open(f\"nvidia_stock_data_comments{start_date.strftime('%Y_%m_%d')}_{end_date.strftime('%Y_%m_%d')}.json\", \"w\") as json_file:\n",
        "        json.dump(comments_json, json_file)\n",
        "    print(\"Comments Data saved to nvidia_stock_data_comments.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk5-q5gXzydn"
      },
      "source": [
        "### Run the above function to scrape reddit data between start_date and end_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Zpvx8jIG1t",
        "outputId": "db9f6749-5698-43bb-8230-ce6bf961b005"
      },
      "outputs": [],
      "source": [
        "start_date = datetime.datetime(2024, 8, 1, 0, 0, 0)  # Year, Month, Day, Hour, Minute, Second\n",
        "end_date = datetime.datetime(2024, 9, 2, 0, 5, 0)\n",
        "# reddit_api_response(sSubredditName, hotLimit,start_date, end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxZFQMEQz_-a"
      },
      "source": [
        "### print top 3 posts overall and on 28 August and plot number of posts per date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OEvwcOiqkuTf",
        "outputId": "530aa2b5-0c12-41c7-c7cc-1dc9a152da3e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON data from the file\n",
        "with open('nvidia_stock_data_2024_08_01_2024_09_02.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "# Find top 3 posts on August 28th\n",
        "target_date = datetime.datetime(2024, 8, 28).date()\n",
        "posts_on_target_date = []\n",
        "\n",
        "for post in data['submissions']:\n",
        "    post_date = datetime.datetime.fromtimestamp(post['created']).date()\n",
        "    if post_date == target_date:\n",
        "        posts_on_target_date.append(post)\n",
        "\n",
        "# Sort posts by score in descending order and get top 3\n",
        "top_3_posts_aug_28 = sorted(posts_on_target_date, key=lambda x: x.get('score', 0), reverse=True)[:3]\n",
        "\n",
        "print(\"Top 3 posts on August 28th:\")\n",
        "for i, post in enumerate(top_3_posts_aug_28, 1):\n",
        "    print(f\"\\n{i}. Score: {post.get('score', 'N/A')}\")\n",
        "    print(f\"   Title: {post.get('title', 'N/A')[:150]}...\")  # Print first 100 characters of the title\n",
        "    print(f\"   Author: {post.get('author', 'N/A')}\")\n",
        "    print(f\"   Created: {datetime.datetime.fromtimestamp(post['created'])}\")\n",
        "\n",
        "# Find top 3 posts overall\n",
        "all_posts = data['submissions']\n",
        "top_3_posts_overall = sorted(all_posts, key=lambda x: x.get('score', 0), reverse=True)[:3]\n",
        "\n",
        "print(\"\\nTop 3 posts overall:\")\n",
        "for i, post in enumerate(top_3_posts_overall, 1):\n",
        "    print(f\"\\n{i}. Score: {post.get('score', 'N/A')}\")\n",
        "    print(f\"   Title: {post.get('title', 'N/A')[:150]}...\")  # Print first 100 characters of the title\n",
        "    print(f\"   Author: {post.get('author', 'N/A')}\")\n",
        "    print(f\"   Created: {datetime.datetime.fromtimestamp(post['created'])}\")\n",
        "# Create a DataFrame from the comments\n",
        "df = pd.DataFrame(data['submissions'])\n",
        "\n",
        "# Convert the 'date' column to datetime\n",
        "df['date'] = pd.to_datetime(df['created'], unit='s')\n",
        "\n",
        "# Group by date and count the number of posts for each date\n",
        "comments_per_date = df.groupby(df['date'].dt.date).size()\n",
        "\n",
        "# Print the number of posts for each date\n",
        "print(comments_per_date)\n",
        "# Plot the number of posts for each date\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "comments_per_date.plot()\n",
        "plt.scatter(comments_per_date.index, comments_per_date.values, color='black')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of posts')\n",
        "plt.title('Number of posts per date')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydu-g8uc0RJW"
      },
      "source": [
        "### print top 3 comments overall and on 28 August and plot number of comments per date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fhCGsdlUkuTf",
        "outputId": "b2c75576-21d3-4cdd-ea51-0ef887be469f"
      },
      "outputs": [],
      "source": [
        "# Load the JSON data from the file\n",
        "with open('nvidia_stock_data_comments2024_08_01_2024_09_02.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "# Find top 5 comments on August 28th\n",
        "target_date = datetime.datetime(2024, 8, 28).date()\n",
        "comments_on_target_date = []\n",
        "\n",
        "for comment in data['submissions']:\n",
        "    comment_date = datetime.datetime.fromtimestamp(comment['created']).date()\n",
        "    if comment_date == target_date:\n",
        "        comments_on_target_date.append(comment)\n",
        "\n",
        "# Sort comments by score in descending order and get top 3\n",
        "top_5_comments = sorted(comments_on_target_date, key=lambda x: x.get('score', 0), reverse=True)[:3]\n",
        "\n",
        "print(\"Top 3 comments on August 28th:\")\n",
        "for i, comment in enumerate(top_5_comments, 1):\n",
        "    print(f\"\\n{i}. Score: {comment.get('score', 'N/A')}\")\n",
        "    print(f\"   Author: {comment.get('author', 'N/A')}\")\n",
        "    print(f\"   Created: {datetime.datetime.fromtimestamp(comment['created'])}\")\n",
        "    print(f\"   Text: {comment.get('text', 'N/A')[:150]}...\")  # Print first 100 characters of the comment\n",
        "\n",
        "# Find top 3 comments overall\n",
        "all_comments = data['submissions']\n",
        "top_3_comments_overall = sorted(all_comments, key=lambda x: x.get('score', 0), reverse=True)[:3]\n",
        "\n",
        "print(\"\\nTop 3 comments overall:\")\n",
        "for i, comment in enumerate(top_3_comments_overall, 1):\n",
        "    print(f\"\\n{i}. Score: {comment.get('score', 'N/A')}\")\n",
        "    print(f\"   Author: {comment.get('author', 'N/A')}\")\n",
        "    print(f\"   Created: {datetime.datetime.fromtimestamp(comment['created'])}\")\n",
        "    print(f\"   Text: {comment.get('text', 'N/A')[:150]}...\")  # Print first 150 characters of the comment\n",
        "\n",
        "# Create a DataFrame from the comments\n",
        "df = pd.DataFrame(data['submissions'])\n",
        "\n",
        "# Convert the 'date' column to datetime\n",
        "df['date'] = pd.to_datetime(df['created'], unit='s')\n",
        "\n",
        "# Group by date and count the number of comments for each date\n",
        "comments_per_date = df.groupby(df['date'].dt.date).size()\n",
        "\n",
        "# Print the number of comments for each date\n",
        "print(comments_per_date)\n",
        "# Plot the number of comments for each date\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "comments_per_date.plot()\n",
        "plt.scatter(comments_per_date.index, comments_per_date.values, color='black')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of comments')\n",
        "plt.title('Number of comments per date')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOxB4AzhS6s3"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCnEMRP5VJ2S",
        "outputId": "a5ec9932-09c8-434a-ada2-6a6d04c80336"
      },
      "outputs": [],
      "source": [
        "def processText(text, abbreviations, tokenizer, n_grams, lemmatizer, stopwords):\n",
        "    \"\"\"\n",
        "    Process the text by tokenizing, lemmatizing, and removing stopwords.\n",
        "\n",
        "    @param text: The text to process\n",
        "    @param abbreviations: Dictionary of abbreviations\n",
        "    @param tokenizer: Tokenizer to use\n",
        "    @param n_grams: Number of n-grams\n",
        "    @param lemmatizer: Lemmatizer to use\n",
        "    @param stopwords: List of stopwords\n",
        "\n",
        "    @returns: A list of processed tokens\n",
        "    \"\"\"\n",
        "    # Remove URLs and other patterns\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Convert all tokens to lowercase\n",
        "    tokens = [item.lower() for item in tokens]\n",
        "\n",
        "    # Replace abbreviations\n",
        "    tokens = [abbreviations.get(token, token) for token in tokens]\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if not token.isdigit() and token not in stopwords]\n",
        "\n",
        "    # Generate n-grams if n_grams > 1\n",
        "    if n_grams > 1:\n",
        "        tokens = list(nltk.ngrams(tokens, n_grams))\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bemw3o5Q0eAu"
      },
      "source": [
        "### Do parallel processing to speed up code and save tokens to .txt files and make term frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgwrEgjrVP6z",
        "outputId": "e75b7393-d816-4f27-be60-50f7c868b0a7"
      },
      "outputs": [],
      "source": [
        "# Function to process each submission\n",
        "def process_submission(submission, abbreviations, tokenizer, n_grams, lemmatizer, stopwords):\n",
        "    submissionsTitle = submission.get('text', '')\n",
        "    lTokens = processText(text=submissionsTitle, abbreviations=abbreviations, tokenizer=tokenizer, n_grams=n_grams, lemmatizer=lemmatizer, stopwords=stopwords)\n",
        "    return lTokens\n",
        "\n",
        "# Function to handle parallel processing\n",
        "def process_submissions(submission, abbreviations, tokenizer, n_grams, lemmatizer, stopwords):\n",
        "    return process_submission(submission, abbreviations, tokenizer, n_grams, lemmatizer, stopwords)\n",
        "\n",
        "# Load JSON file\n",
        "fJsonName = f\"nvidia_stock_data_comments{start_date.strftime('%Y_%m_%d')}_{end_date.strftime('%Y_%m_%d')}.json\"\n",
        "freqNum = 100\n",
        "\n",
        "# Tweet tokenizer to use\n",
        "tweetTokeniser = nltk.tokenize.TweetTokenizer()\n",
        "# Use the punctuation symbols defined in string.punctuation\n",
        "lPunct = list(string.punctuation)\n",
        "# Use stopwords from nltk and a few other Twitter-specific terms like 'rt' (retweet)\n",
        "lStopwords = nltk.corpus.stopwords.words('english') + lPunct + ['via'] + [\"’\"] + [\"...\"] + [\"…\"]\n",
        "\n",
        "redditLemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Our term frequency counter\n",
        "termFreqCounter = Counter()\n",
        "n_grams = 1\n",
        "\n",
        "abbreviations = {}\n",
        "ltokens = {}\n",
        "\n",
        "# Check if the file exists and is not empty\n",
        "if os.path.exists('abbreviations.txt') and os.path.getsize('abbreviations.txt') > 0:\n",
        "    with open('abbreviations.txt') as json_file:\n",
        "        abbreviations = json.load(json_file)\n",
        "\n",
        "# Open JSON file and process it tweet by tweet using ijson for iterative parsing\n",
        "with open(fJsonName, 'r') as f:\n",
        "    dSubmissions = ijson.items(f, 'submissions.item')\n",
        "\n",
        "    # Parallel processing of submissions\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        results = list(executor.map(process_submissions, dSubmissions, repeat(abbreviations), repeat(tweetTokeniser), repeat(n_grams), repeat(redditLemmatizer), repeat(lStopwords)))\n",
        "    # Determine the output file name based on fJsonName\n",
        "    if \"comments\" in fJsonName:\n",
        "        output_file_name = 'submission_tokens_comments.txt'\n",
        "    else:\n",
        "        output_file_name = 'submission_tokens_post.txt'\n",
        "\n",
        "    # Update term frequency counter and save tokens to file\n",
        "    with open(output_file_name, 'a') as f:\n",
        "        for lTokens in results:\n",
        "            f.write(' '.join(lTokens) + '\\n')\n",
        "            termFreqCounter.update(lTokens)\n",
        "\n",
        "freq_terms = []\n",
        "# Print out most common terms\n",
        "for term, count in termFreqCounter.most_common(freqNum):\n",
        "    freq_terms.append(term)\n",
        "    freq_terms.append(count)\n",
        "    print(term + ': ' + str(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "DmRkJTNzYVuL",
        "outputId": "d71544a1-41d9-4e95-c7bb-bc5f6c40e62e"
      },
      "outputs": [],
      "source": [
        "def plot_term_frequencies(freq_terms, top_n=20, title='Frequency of Terms'):\n",
        "    \"\"\"\n",
        "    Plots the frequency of terms.\n",
        "\n",
        "    Parameters:\n",
        "    freq_terms (list): A list of terms and their frequencies in alternating order.\n",
        "    top_n (int): The number of top terms to display in the plot. Default is 20.\n",
        "    title (str): The title of the plot.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    values = []\n",
        "\n",
        "    # Iterate through the list\n",
        "    for i in range(0, len(freq_terms), 2):\n",
        "        labels.append(freq_terms[i])      # Add the string to labels list\n",
        "        values.append(freq_terms[i+1])    # Add the number to values list\n",
        "\n",
        "    plt.figure(figsize=(10, 6))  # Adjust the figure size for better readability\n",
        "    plt.bar(labels[:top_n], values[:top_n])\n",
        "\n",
        "    # Rotate x-axis labels for better visibility\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add title and labels\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Terms')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Adjust layout to ensure everything fits\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "    #     # Join the different processed titles together.\n",
        "plot_term_frequencies(freq_terms, top_n=20, title='Frequency of Terms for processed posts')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG_NCyPl08ty"
      },
      "source": [
        "### Function to determine term frequency of raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UMVZs8WakuTh",
        "outputId": "9b8974b9-a2c9-47f3-dc9d-0bb035ca9ca3"
      },
      "outputs": [],
      "source": [
        "def onlyprocessText(text):\n",
        "    # tweet tokeniser to use\n",
        "    tweetTokeniser = nltk.tokenize.TweetTokenizer()\n",
        "    lTokens = tweetTokeniser.tokenize(text)\n",
        "    return lTokens\n",
        "\n",
        "# Function to process each submission\n",
        "def process_submission(submission):\n",
        "    submissionsTitle = submission.get('text', '') or submission.get('title', '')\n",
        "    lTokens = onlyprocessText(text=submissionsTitle)\n",
        "    return lTokens\n",
        "\n",
        "\n",
        "# Function to handle parallel processing\n",
        "def process_submissions(submission):\n",
        "    return process_submission(submission)\n",
        "\n",
        "# Our term frequency counter\n",
        "termFreqCounter = Counter()\n",
        "\n",
        "# Open JSON file and process it tweet by tweet using ijson for iterative parsing\n",
        "with open(fJsonName, 'r') as f:\n",
        "    dSubmissions = ijson.items(f, 'submissions.item')\n",
        "\n",
        "    # Parallel processing of submissions\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        results = list(executor.map(process_submissions, dSubmissions))\n",
        "\n",
        "    # Update term frequency counter and save tokens to file\n",
        "    with open('submission_tokens_raw.txt', 'a') as f:\n",
        "        for lTokens in results:\n",
        "            f.write(' '.join(lTokens) + '\\n')\n",
        "            termFreqCounter.update(lTokens)\n",
        "\n",
        "freq_terms = []\n",
        "# Print out most common terms\n",
        "for term, count in termFreqCounter.most_common(freqNum):\n",
        "    freq_terms.append(term)\n",
        "    freq_terms.append(count)\n",
        "    print(term + ': ' + str(count))\n",
        "\n",
        "plot_term_frequencies(freq_terms, top_n=20, title='Frequency of Terms for raw comments')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVvFfNp_1FZj"
      },
      "source": [
        "### combine posts and comment tokens to one list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvXZI2frLKLy",
        "outputId": "abfe7901-369c-4694-9f05-f2bf27ef0201"
      },
      "outputs": [],
      "source": [
        "# Read the contents of submission_tokens.txt and convert it to a list where each line is an element of the list as a string\n",
        "\n",
        "with open('submission_tokens_posts.txt', 'r') as file:\n",
        "    lPost = file.read().splitlines()\n",
        "with open('submission_tokens_comments.txt', 'r') as file:\n",
        "    lPosts_comments = file.read().splitlines()\n",
        "lPosts = lPost + lPosts_comments\n",
        "# Print the list to verify\n",
        "print(lPosts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NNfXQvbY5_e"
      },
      "source": [
        "### LDA Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQRHIg4y1RBI"
      },
      "source": [
        "### Plotting coherence term plot to determine optimal number of topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "l5EDx0vOyMIS",
        "outputId": "5bcf60ae-7e78-48e7-f701-bcb5d030105e"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import numpy as np\n",
        "\n",
        "# Read and combine the data (this part remains the same)\n",
        "with open('submission_tokens_posts.txt', 'r') as file:\n",
        "    lPost = file.read().splitlines()\n",
        "with open('submission_tokens_comments.txt', 'r') as file:\n",
        "    lPosts_comments = file.read().splitlines()\n",
        "\n",
        "lPosts = lPost + lPosts_comments\n",
        "data = [post.split() for post in lPosts]\n",
        "\n",
        "# Create Dictionary and Corpus (this part remains the same)\n",
        "id2word = corpora.Dictionary(data)\n",
        "texts = data\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "def compute_coherence_for_model(params):\n",
        "    \"\"\"Compute coherence for a single model\"\"\"\n",
        "    num_topics, dictionary, corpus, texts = params\n",
        "    model = gensim.models.LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=100,\n",
        "        update_every=1,\n",
        "        chunksize=100,\n",
        "        passes=10,\n",
        "        alpha='auto',\n",
        "        per_word_topics=True\n",
        "    )\n",
        "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
        "    \"\"\"Compute coherence values for multiple models in parallel\"\"\"\n",
        "    pool = Pool(processes=cpu_count())\n",
        "    coherence_values = pool.map(\n",
        "        compute_coherence_for_model,\n",
        "        [(num_topics, dictionary, corpus, texts) for num_topics in range(start, limit, step)]\n",
        "    )\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return coherence_values\n",
        "\n",
        "# Set parameters\n",
        "start, limit, step = 2, 40, 6\n",
        "\n",
        "# Compute coherence values\n",
        "coherence_values = compute_coherence_values(id2word, corpus, texts, start, limit, step)\n",
        "\n",
        "# Plot results\n",
        "x = range(start, limit, step)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x, coherence_values, 'bo-')\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence Score\")\n",
        "plt.title(\"Topic Coherence Scores by Number of Topics\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal number of topics\n",
        "optimal_num_topics = x[np.argmax(coherence_values)]\n",
        "print(f'Optimal number of topics: {optimal_num_topics}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXzR-KrpcVP9"
      },
      "outputs": [],
      "source": [
        "# number of topics to discover (default = 10)\n",
        "topicNum = 2\n",
        "# maximum number of words to display per topic (default = 10)\n",
        "# Answer to Exercise 1 (change from 10 to 15)\n",
        "wordNumToDisplay = 5\n",
        "# this is the number of features/words to used to describe our documents\n",
        "# please feel free to change to see effect\n",
        "featureNum = 1500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0qzrd7ZQ9Z"
      },
      "source": [
        "Performs counting via CountVectorizer and then apply the LDA model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWVxnYLpccrd"
      },
      "outputs": [],
      "source": [
        "tfVectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=featureNum, stop_words='english')\n",
        "tf = tfVectorizer.fit_transform(lPosts)\n",
        "# extract the names of the features (in our case, the words)\n",
        "tfFeatureNames = tfVectorizer.get_feature_names_out()\n",
        "\n",
        "ldaModel = LatentDirichletAllocation(n_components =topicNum, max_iter=10, learning_method='online').fit(tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apuiw6yZpJa"
      },
      "source": [
        "### Visualising topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNwmOe-2cfDr"
      },
      "outputs": [],
      "source": [
        "def display_topics(model, featureNames, numTopWords):\n",
        "    \"\"\"\n",
        "    Prints out the most associated words for each feature.\n",
        "\n",
        "    @param model: lda model.\n",
        "    @param featureNames: list of strings, representing the list of features/words.\n",
        "    @param numTopWords: number of words to print per topic.\n",
        "    \"\"\"\n",
        "\n",
        "    # print out the topic distributions\n",
        "    for topicId, lTopicDist in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topicId))\n",
        "        print(\" \".join([featureNames[i] for i in lTopicDist.argsort()[:-numTopWords - 1:-1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pey4lTQ4ZwrT"
      },
      "source": [
        "Diplays discovered topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3VW1MmBchm-"
      },
      "outputs": [],
      "source": [
        "display_topics(ldaModel, tfFeatureNames, wordNumToDisplay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctVlrBfjZ3fN"
      },
      "source": [
        "### pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUigA2X-cj71"
      },
      "outputs": [],
      "source": [
        "# TODO: Add the pyLDAvis code here\n",
        "# note if you also implemented the word cloud, that will display first, then once you close that\n",
        "# file, then this will display\n",
        "# Answer to exercise 2\n",
        "panel = pyLDAvis.lda_model.prepare(ldaModel, tf, tfVectorizer, mds='tsne')\n",
        "pyLDAvis.display(panel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07VbuEJzZ9dx"
      },
      "source": [
        "### Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2hd4vwwaAPD"
      },
      "outputs": [],
      "source": [
        "def displayWordcloud(model, featureNames):\n",
        "    \"\"\"\n",
        "    Displays the word cloud of the topic distributions, stored in model.\n",
        "\n",
        "    @param model: lda model.\n",
        "    @param featureNames: list of strings, representing the list of features/words.\n",
        "    \"\"\"\n",
        "\n",
        "    # this normalises each row/topic to sum to one\n",
        "    # use this normalisedComponents to display your wordclouds\n",
        "    normalisedComponents = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # TODO: complete the implementation\n",
        "\n",
        "    #\n",
        "    # Answer to Exercises 3 and 4\n",
        "    #\n",
        "\n",
        "    topicNum = len(model.components_)\n",
        "    # number of wordclouds for each row\n",
        "    plotColNum = 3\n",
        "    # number of wordclouds for each column\n",
        "    plotRowNum = int(math.ceil(topicNum / plotColNum))\n",
        "\n",
        "    for topicId, lTopicDist in enumerate(normalisedComponents):\n",
        "        lWordProb = {featureNames[i] : wordProb for i,wordProb in enumerate(lTopicDist)}\n",
        "        wordcloud = WordCloud(background_color='black')\n",
        "        wordcloud.fit_words(frequencies=lWordProb)\n",
        "        plt.subplot(plotRowNum, plotColNum, topicId+1)\n",
        "        plt.title('Topic %d:' % (topicId+1))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show(block=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea_DfoDohdgL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import os\n",
        "\n",
        "def displayWordcloud(lda_model, tf_feature_names, num_words=10, output_dir='wordclouds'):\n",
        "    \"\"\"\n",
        "    Display and save word clouds for each topic in the LDA model.\n",
        "\n",
        "    @param lda_model: Trained LDA model\n",
        "    @param tf_feature_names: List of feature names (terms)\n",
        "    @param num_words: Number of words to display in the word cloud\n",
        "    @param output_dir: Directory to save the word cloud images\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        # Generate word cloud\n",
        "        wordcloud = WordCloud(background_color='white', width=800, height=400, max_words=num_words)\n",
        "        wordcloud.generate_from_frequencies(dict(zip(tf_feature_names, topic)))\n",
        "\n",
        "        # Display the word cloud\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Topic #{topic_idx}')\n",
        "        plt.show()\n",
        "\n",
        "        # Save the word cloud image\n",
        "        wordcloud.to_file(os.path.join(output_dir, f'topic_{topic_idx}.png'))\n",
        "\n",
        "# Example usage\n",
        "# Assuming lda_model and tf_feature_names are already defined\n",
        "displayWordcloud(ldaModel, tfFeatureNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_fQViFAaCxH"
      },
      "outputs": [],
      "source": [
        "# display wordcloud\n",
        "# TODO: go to the function definition and complete its implementation\n",
        "displayWordcloud(ldaModel, tfFeatureNames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZr5KV5aoE_2"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIO1EEDJoQ6k"
      },
      "source": [
        "## Use count method to calculate the sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMSFlXDtoI1I"
      },
      "outputs": [],
      "source": [
        "def computeSentiment(lTokens, setPosWords, setNegWords):\n",
        "    \"\"\"\n",
        "    Compute the overall sentiment of the list of tokens in lTokens, using the countWordSentimentAnalysis approach.\n",
        "\n",
        "    @param lTokens: List of tokens to calculate the overall sentiment for.\n",
        "    @param setPosWords: Set of positive words.\n",
        "    @param setNegWords: Set of negative words.\n",
        "\n",
        "    @returns Sentiment score for lTokens.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    posNum = len([tok for tok in lTokens if tok in setPosWords])\n",
        "\n",
        "    negNum = len([tok for tok in lTokens if tok in setNegWords])\n",
        "    sentiment = posNum - negNum\n",
        "    return sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glq2xZElolFy"
      },
      "outputs": [],
      "source": [
        "def printColouredTokens(lTokens, setPosWords, setNegWords, sentiment):\n",
        "    \"\"\"\n",
        "    Print out the tokens in different colours, according to sentiment.\n",
        "    If positive, in red.\n",
        "    If negative, in blue.\n",
        "    Otherwise no colouring.\n",
        "\n",
        "    @param lTokens: List of tokens to print and colour.\n",
        "    @param setPosWords: Set of positive words.\n",
        "    @param setNegWords: Set of negative words.\n",
        "    @param sentiment: Sentiment score of list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    for token in lTokens:\n",
        "        if token in setPosWords:\n",
        "            print(Fore.RED + token + ', ', end='')\n",
        "        elif token in setNegWords:\n",
        "            print(Fore.BLUE + token + ', ', end='')\n",
        "        else:\n",
        "            print(Style.RESET_ALL + token + ', ', end='')\n",
        "\n",
        "    print(': {}'.format(sentiment))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4kevpJuomw0"
      },
      "outputs": [],
      "source": [
        "def countWordSentimentAnalysis(setPosWords, setNegWords, sTweetsFilename, bPrint):\n",
        "    \"\"\"\n",
        "    Basic sentiment analysis.  Count the number of positive words, count the negative words, overall polarity is the\n",
        "    difference in the two numbers.\n",
        "\n",
        "    @param setPosWords: set of positive sentiment words\n",
        "    @param setNegWords: set of negative sentiment words\n",
        "    @param sFilename: name of input file containing a json formated dump\n",
        "    @param bPrint: whether to print the stream of tokens and sentiment.  Uses colorama to highlight sentiment words.\n",
        "    @param redditProcessor: RedditProcessing object, used to pre-process each tweet.\n",
        "\n",
        "    @returns: list of reddit posts, in the format of [date, sentiment]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    lSentiment = []\n",
        "    # open file and process reddit submissions and comments, one by one\n",
        "    with open(sTweetsFilename, 'r') as f:\n",
        "        redditDump = json.load(f)\n",
        "\n",
        "        for submission in redditDump['submissions']:\n",
        "              if \"comments\" in fJsonName:\n",
        "                postText = submission[\"text\"]\n",
        "              else:\n",
        "                postText = submission['title']\n",
        "\n",
        "              postDate = submission['created']\n",
        "\n",
        "              # tokenise, filter stopwords and get convert to lower case\n",
        "              lTokens = processText(text=postText, abbreviations = abbreviations, tokenizer=tweetTokeniser, n_grams=n_grams, lemmatizer=redditLemmatizer, stopwords=lStopwords)\n",
        "\n",
        "              # from where this redditProcessor comes from?\n",
        "\n",
        "              # compute sentiment\n",
        "              sentiment = computeSentiment(lTokens, setPosWords, setNegWords)\n",
        "\n",
        "              # save the date and sentiment of each reddit post (used for time series)\n",
        "              lSentiment.append([pd.to_datetime(postDate, unit='s'), sentiment])\n",
        "\n",
        "              # if we are printing, each token is printed and coloured according to red if positive word, and blue\n",
        "              # if negative\n",
        "              if bPrint:\n",
        "                  printColouredTokens(lTokens, setPosWords, setNegWords, sentiment)\n",
        "    return lSentiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZeo1DafouK1"
      },
      "source": [
        "### Approach 2: Vader based approach to sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bes_ZsB5oq_z"
      },
      "outputs": [],
      "source": [
        "def vaderPrintTokens(lTokens, dSentimentScores):\n",
        "    \"\"\"\n",
        "    Print out the tokens and sentiment score.\n",
        "\n",
        "    @param lTokens: List of tokens to print and colour.\n",
        "    @dSentimentScores: Dictionary of sentiment from Vader.\n",
        "\n",
        "    \"\"\"\n",
        "    print(*lTokens, sep=', ')\n",
        "    for cat,score in dSentimentScores.items():\n",
        "        print('{0}: {1}, '.format(cat, score), end='')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi9SFZpGoxEq"
      },
      "outputs": [],
      "source": [
        "def vaderSentimentAnalysis(sTweetsFilename, bPrint):\n",
        "    \"\"\"\n",
        "    Use Vader lexicons instead of a raw positive and negative word count.\n",
        "\n",
        "    @param sTweetsFilename: name of input file containing a json formated tweet dump\n",
        "    @param bPrint: whether to print the stream of tokens and sentiment.  Uses colorama to highlight sentiment words.\n",
        "    @param tweetProcessor: TweetProcessing object, used to pre-process each tweet.\n",
        "\n",
        "    @returns: list of tweets, in the format of [date, sentiment]\n",
        "    \"\"\"\n",
        "\n",
        "    # this is the vader sentiment analyser, part of nltk\n",
        "    sentAnalyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "    lSentiment = []\n",
        "    # open file and process tweets, one by one\n",
        "    with open(sTweetsFilename, 'r') as f:\n",
        "        redditDump = json.load(f)\n",
        "\n",
        "        for submission in redditDump['submissions']:\n",
        "              if \"comments\" in fJsonName:\n",
        "                postText = submission[\"text\"]\n",
        "              else:\n",
        "                postText = submission['title']\n",
        "              postDate = submission['created']\n",
        "\n",
        "              # tokenise, filter stopwords and get convert to lower case\n",
        "              lTokens = processText(text=postText, abbreviations = abbreviations, tokenizer=tweetTokeniser, n_grams=n_grams, lemmatizer=redditLemmatizer, stopwords=lStopwords)\n",
        "\n",
        "\n",
        "              # this computes the sentiment scores (called polarity score in nltk, but mean same thing essentially)\n",
        "              # see workshop sheet for what dSentimentScores holds\n",
        "              dSentimentScores = sentAnalyser.polarity_scores(\" \".join(lTokens))\n",
        "\n",
        "              # save the date and sentiment of each post (used for time series)\n",
        "              lSentiment.append([pd.to_datetime(postDate, unit='s'), dSentimentScores['compound']])\n",
        "\n",
        "              # if we are printing, we print the tokens then the sentiment scores.  Because we don't have the list\n",
        "              # of positive and negative words, we cannot use colorama to label each token\n",
        "              if bPrint:\n",
        "                  vaderPrintTokens(lTokens, dSentimentScores)\n",
        "\n",
        "    return lSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeYvFaBso50p"
      },
      "outputs": [],
      "source": [
        "# input file of set of postive words\n",
        "posWordFile = 'positive-words.txt'\n",
        "# input file of set of negative words\n",
        "negWordFile = 'negative-words.txt'\n",
        "# input file of set of reddit posts (json format)\n",
        "# redditFile = 'nvidia_stock_data_1_11_2015_13_8_2024.json'\n",
        "# flag to determine whether to print out tweets and their sentiment\n",
        "flagPrint = True\n",
        "# specify the approach to take, one of [count, vader]\n",
        "# change this to use a different sentiment approach\n",
        "approach = 'count'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kkl4MS6o9a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load set of positive words\n",
        "lPosWords = []\n",
        "with open(posWordFile, 'r', encoding='utf-8', errors='ignore') as fPos:\n",
        "    for sLine in fPos:\n",
        "        lPosWords.append(sLine.strip())\n",
        "\n",
        "setPosWords = set(lPosWords)\n",
        "\n",
        "\n",
        "# load set of negative words\n",
        "lNegWords = []\n",
        "with codecs.open(negWordFile, 'r', encoding='utf-8', errors='ignore') as fNeg:\n",
        "    for sLine in fNeg:\n",
        "        lNegWords.append(sLine.strip())\n",
        "\n",
        "setNegWords = set(lNegWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-24z9KHkpBXP"
      },
      "outputs": [],
      "source": [
        "# compute the sentiment\n",
        "# to change method, update parameter settings, particularly the variable 'approach' and rerun the parameter setting cell,\n",
        "# and also this cell\n",
        "lSentiment = []\n",
        "if approach == 'count':\n",
        "    lSentiment = countWordSentimentAnalysis(setPosWords, setNegWords, fJsonName, flagPrint)\n",
        "elif approach == 'vader':\n",
        "    lSentiment = vaderSentimentAnalysis(fJsonName, flagPrint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTwcCUcFpEyN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# we are using pandas for this, but first we need to get it into a pandas data frame structure\n",
        "series = pd.DataFrame(lSentiment, columns=['date', 'sentiment'])\n",
        "# tell pandas that the date column is the one we use for indexing (or x-axis)\n",
        "series.set_index('date', inplace=True)\n",
        "# pandas makes a guess at the type of the columns, but to make sure it doesn't get it wrong, we set the sentiment\n",
        "# column to floats\n",
        "series[['sentiment']] = series[['sentiment']].apply(pd.to_numeric)\n",
        "\n",
        "newSeries = series.resample('1D').sum()\n",
        "# this plots and shows the time series\n",
        "# Plot the time series with enhanced appearance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(newSeries, marker='o', linestyle='-', color='b')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Daily Sentiment Analysis of comments using Count Method', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Sentiment Score', fontsize=14)\n",
        "\n",
        "# Add grid\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
