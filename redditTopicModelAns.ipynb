{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSC2671 Social Media and Network Analytics\n",
    "### Workshop 6: Topic Models\n",
    "\n",
    "#### Jeffrey Chan, RMIT University, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This automatically reloads the client information if there are changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport redditClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redditClient import redditClient\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import math\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text, tokenizer, stemmer, stopwords):\n",
    "    \"\"\"\n",
    "    Perform tokenisation, normalisation (lower case and stemming) and stopword and twitter keyword removal.\n",
    "\n",
    "    @param text: reddit submission or comment text\n",
    "    @param tokenizer: tokeniser used.\n",
    "    @param stemmer: stemmer used.\n",
    "    @param stopwords: list of stopwords used\n",
    "\n",
    "    @returns: a list of processed tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # covert all to lower case\n",
    "    text = text.lower()\n",
    "    # tokenise\n",
    "    lTokens = tokenizer.tokenize(text)\n",
    "    # strip whitespaces before and after\n",
    "    lTokens = [token.strip() for token in lTokens]\n",
    "    # stem (we use set to remove duplicates)\n",
    "    lStemmedTokens = set([stemmer.stem(tok) for tok in lTokens])\n",
    "\n",
    "\n",
    "    # remove stopwords, digits\n",
    "    return [tok for tok in lStemmedTokens if tok not in stopwords and not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of own timeline, will retrieve the specified user's timeline \n",
    "sSubredditName = 'RMIT'\n",
    "\n",
    "# number of hot submissions to retrieve\n",
    "hotLimit = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Reddit client\n",
    "client = redditClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet to use\n",
    "postTokeniser = nltk.tokenize.TweetTokenizer()\n",
    "# use the punctuation symbols defined in string.punctuation\n",
    "lPunct = list(string.punctuation)\n",
    "# use stopwords from nltk and a few other twitter specific terms like 'rt' (retweet)\n",
    "lStopwords = nltk.corpus.stopwords.words('english') + lPunct + ['via']\n",
    "# we use the popular Porter stemmer\n",
    "postStemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# specify which subreddit we are interested in\n",
    "subreddit = client.subreddit(sSubredditName)\n",
    "\n",
    "\n",
    "# this will store the list of posts we read from subreddit\n",
    "lPosts = []\n",
    "\n",
    "# print out the hot submisisons\n",
    "for submission in subreddit.hot(limit=hotLimit):\n",
    "    # print title, but we can print other information as well\n",
    "    submissionTitle = submission.title\n",
    "    # tokenise, filter stopwords and get convert to lower case\n",
    "    lTokens = processText(text=submissionTitle, tokenizer=postTokeniser, stemmer=postStemmer, stopwords=lStopwords)\n",
    "    lPosts.append(' '.join(lTokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# LDA parameters\n",
    "#\n",
    "\n",
    "# number of topics to discover (default = 10)\n",
    "topicNum = 10\n",
    "# maximum number of words to display per topic (default = 10)\n",
    "# Answer to Exercise 1 (change from 10 to 15)\n",
    "wordNumToDisplay = 15\n",
    "# this is the number of features/words to used to describe our documents\n",
    "# please feel free to change to see effect\n",
    "featureNum = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs counting via CountVectorizer and then apply the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count Vectorizer\n",
    "#\n",
    "\n",
    "tfVectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=featureNum, stop_words='english')\n",
    "tf = tfVectorizer.fit_transform(lPosts)\n",
    "# extract the names of the features (in our case, the words)\n",
    "tfFeatureNames = tfVectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "#\n",
    "# LDA MODEL\n",
    "#\n",
    "\n",
    "# Run LDA (see documentation about what the arguments means)\n",
    "ldaModel = LatentDirichletAllocation(n_components =topicNum, max_iter=10, learning_method='online').fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, featureNames, numTopWords):\n",
    "    \"\"\"\n",
    "    Prints out the most associated words for each feature.\n",
    "\n",
    "    @param model: lda model.\n",
    "    @param featureNames: list of strings, representing the list of features/words.\n",
    "    @param numTopWords: number of words to print per topic.\n",
    "    \"\"\"\n",
    "\n",
    "    # print out the topic distributions\n",
    "    for topicId, lTopicDist in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topicId))\n",
    "        print(\" \".join([featureNames[i] for i in lTopicDist.argsort()[:-numTopWords - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diplays discovered topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "anim bachelor help select task portfolio club thi need uni law account art advanc week\n",
      "Topic 1:\n",
      "studi student look fee friend rmit anyon market code ani social master transfer mani assign\n",
      "Topic 2:\n",
      "need laptop code help access law program doe question ani transfer anyon rmit account student\n",
      "Topic 3:\n",
      "account law bachelor degre like question regard scholarship transfer applic student thi anim design code\n",
      "Topic 4:\n",
      "student scholarship design advanc diploma question ani game portfolio bundoora degre help engin studi write\n",
      "Topic 5:\n",
      "cours semest thi engin fine internship art write week help game law access code comput\n",
      "Topic 6:\n",
      "applic select cours uni master help need advic law market thi design transfer assign engin\n",
      "Topic 7:\n",
      "master program assign anyon bootcamp cours drop engin thi social doe advic market help rmit\n",
      "Topic 8:\n",
      "rmit bachelor access advic scienc need comput student fine art club anyon design thi ani\n",
      "Topic 9:\n",
      "uni question rmit need mani classroom regard studi help diploma week cours drop program degre\n"
     ]
    }
   ],
   "source": [
    "display_topics(ldaModel, tfFeatureNames, wordNumToDisplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Add the pyLDAvis code here\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# note if you also implemented the word cloud, that will display first, then once you close that\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# file, then this will display\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Answer to exercise 2\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m panel \u001b[38;5;241m=\u001b[39m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mldaModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfVectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtsne\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(panel)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\reddit\\lib\\site-packages\\pyLDAvis\\lda_model.py:95\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create Prepared Data from sklearn's LatentDirichletAllocation and CountVectorizer.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(_extract_data(lda_model, dtm, vectorizer), kwargs)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\reddit\\lib\\site-packages\\pyLDAvis\\_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[0;32m    430\u001b[0m term_frequency \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(term_topic_freq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 432\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43m_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_term_dists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_topic_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m token_table \u001b[38;5;241m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[0;32m    436\u001b[0m topic_coordinates \u001b[38;5;241m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\reddit\\lib\\site-packages\\pyLDAvis\\_prepare.py:243\u001b[0m, in \u001b[0;36m_topic_info\u001b[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# Order the terms for the \"default\" view by decreasing saliency:\u001b[39;00m\n\u001b[0;32m    237\u001b[0m default_term_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaliency\u001b[39m\u001b[38;5;124m'\u001b[39m: saliency,\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab,\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency,\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefault\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m--> 243\u001b[0m default_term_info \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_term_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaliency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaliency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Rounding Freq and Total to integer values to match LDAvis code:\u001b[39;00m\n\u001b[0;32m    246\u001b[0m default_term_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor(default_term_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: drop() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# TODO: Add the pyLDAvis code here\n",
    "# note if you also implemented the word cloud, that will display first, then once you close that\n",
    "# file, then this will display\n",
    "# Answer to exercise 2\n",
    "\n",
    "panel = pyLDAvis.lda_model.prepare(ldaModel, tf, tfVectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayWordcloud(model, featureNames):\n",
    "    \"\"\"\n",
    "    Displays the word cloud of the topic distributions, stored in model.\n",
    "\n",
    "    @param model: lda model.\n",
    "    @param featureNames: list of strings, representing the list of features/words.\n",
    "    \"\"\"\n",
    "\n",
    "    # this normalises each row/topic to sum to one\n",
    "    # use this normalisedComponents to display your wordclouds\n",
    "    normalisedComponents = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # TODO: complete the implementation\n",
    "    \n",
    "    #\n",
    "    # Answer to Exercises 3 and 4\n",
    "    #\n",
    "    \n",
    "    topicNum = len(model.components_)\n",
    "    # number of wordclouds for each row\n",
    "    plotColNum = 3\n",
    "    # number of wordclouds for each column\n",
    "    plotRowNum = int(math.ceil(topicNum / plotColNum))\n",
    "\n",
    "    for topicId, lTopicDist in enumerate(normalisedComponents):\n",
    "        lWordProb = {featureNames[i] : wordProb for i,wordProb in enumerate(lTopicDist)}\n",
    "        wordcloud = WordCloud(background_color='black')\n",
    "        wordcloud.fit_words(frequencies=lWordProb)\n",
    "        plt.subplot(plotRowNum, plotColNum, topicId+1)\n",
    "        plt.title('Topic %d:' % (topicId+1))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display wordcloud\n",
    "# TODO: go to the function definition and complete its implementation\n",
    "displayWordcloud(ldaModel, tfFeatureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
